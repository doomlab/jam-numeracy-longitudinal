---
title             : "Counting on Memory: How Expertise Shapes Our Numerical Judgments of Associations"
shorttitle        : "COUNTING MEMORY"
author: 
  - name          : "Erin M. Buchanan"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : ""
    email         : "ebuchanan@harrisburgu.edu"
affiliation:
  - id            : "1"
    institution   : "Harrisburg University of Science and Technology"
author_note: |
  
abstract: |
  Accurate numerical estimation underlies many aspects of cognition, from basic quantity judgments to complex decision-making. One domain where numerical reasoning is especially critical is memory, where individuals often must estimate the likelihood that one event or idea is associated with another. In this study, participants completed a free association task across multiple sessions to generate their own individualized word-pair norms. Later, they provided numerical probability judgments (0–100%) of how often they had produced each pair. These judgments were compared to collective free association norms, a matched group evaluating others’ pairs, and a traditional control group. Results showed that participants who judged their own pairs were significantly more accurate in estimating associative probabilities than control or matched groups, reflecting the benefits of expertise derived from repeated interaction with stimuli. However, systematic overestimation bias persisted, especially for weak associations, indicating that metacognitive sensitivity to probability differences remains limited. These findings highlight how expertise improves—but does not perfect—the ability to translate memory associations into numerical judgments, offering new insights into the intersection of numerical cognition, metacognition, and memory.
  
keywords: "numeracy, judgments, memory, expertise"
floatsintext: no
linenumbers: yes
draft: no
mask: no
figurelist: no
tablelist: no
footnotelist: no
classoption: "man"
output: papaja::apa6_pdf
bibliography: references.bib
header-includes:
  - |
    \makeatletter
    \renewcommand{\paragraph}{@startsection{paragraph}{4}{\parindent}%
      {0\baselineskip @plus 0.2ex @minus 0.2ex}%
      {-1em}%
      {\normalfont\normalsize\bfseries\typesectitle}}
      {0\baselineskip @plus 0.2ex @minus 0.2ex}%
      {-\z@\relax}%
      {\normalfont\normalsize\bfseries\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
csl: "`r system.file('rmd', 'apa7.csl', package = 'papaja')`"
documentclass: "apa7"
editor_options: 
  markdown: 
    wrap: sentence
---

```{r load_packages, include = FALSE}
knitr::opts_chunk$set(cache = TRUE)
library("papaja")
library(reshape)
library(nlme)
library(rio)
library(dplyr)
```

People are often asked to make numerical judgments of frequency or probability in daily life.
For instance, if you are waiting for a friend who is late to lunch, you must estimate whether lateness is a high-frequency or low-frequency event to decide when to become concerned.
Such numerical judgments are prone to systematic bias, and research has long shown that people tend to inflate their estimates in many domains.
In education, for example, students often judge their competence at levels higher than their actual performance supports [@Dunning2003].
Similarly, judgments of learning (JOLs) are typically overconfident, which can lead to inefficient study strategies [@Koriat2005].
Individuals who self-monitor their study habits are frequently highly confident but poorly calibrated in their learning [@Cutler1989].
These findings point to a broader problem in cognition: people struggle to map memory experiences onto accurate numerical estimates, underscoring the need for methods that remediate inflated predictions for both theoretical and applied purposes [@Koriat2008; @Koriat2006].
This issue resonates with central questions in numerical cognition, where researchers investigate how humans perceive and evaluate numerical magnitudes, probabilities, and quantities [@dehaene2011; @reyna2008].

Word associations provide a rich domain for studying how people generate and evaluate numerical judgments of probability, making them a useful context for investigating inflated predictions and their potential remediation [@Koriat2006; @Maki2007a].
Typically, word associations are measured using the method of free association [@Nelson2000], in which participants provide the first word (target) that comes to mind when presented with another word (cue).
When aggregated across many individuals, the likelihood that word B follows word A, termed the forward strength (FSG), is expressed as a conditional probability, a fundamentally numerical index of associative strength.
Large-scale databases now provide probabilistic values for thousands of cue–response pairs [@Nelson2004; @dedeyne2019].
For example, the probability that *computer* elicits the response *program* is approximately 12%, meaning that about 12 of 100 people produce that pairing.
Thus, free association tasks translate linguistic memory associations into numerical probability values, making them an ideal bridge between memory research and numerical cognition.

Studies of inflated predictions in word associations consistently show that people overestimate the numerical probability of word relations, particularly for weakly associated pairs [@Maki2007].
This inflation effect represents a systematic bias in probability estimation, paralleling findings in broader numerical cognition where people often misjudge small magnitudes or low-probability events [e.g., @gigerenzer1995; @Reyna2008].
The tendency toward assigning excessively high ratings is remarkably resistant to change [@Maki2007b; @Valentine2013; @Nelson2005].
For example, the inflation persists even when participants are provided with a list of common associates for a given cue [@Foster2012], when they are prompted to consider alternative responses [@Maki2007a], or even when they overtly generate their own list of associates [@Koriat2008; @Koriat2006].
In all these cases, numerical judgments of associative probability remain poorly calibrated, underscoring the robustness of estimation bias across both memory and numerical domains.

Because of the robustness of the inflation effect, finding ways to reduce it is important.
Prior work has shown that mnemonic and theory-based debiasing procedures can reduce overall overestimation bias, but these methods rarely improve sensitivity: the ability to discriminate between low- and high-probability events [\@Valentine2013].
In terms of numerical cognition, *bias* reflects a systematic tendency to assign inflated probability values (treating most associations as stronger than they truly are), whereas *sensitivity* reflects accuracy in tuning numerical judgments to actual associative strength [@Maki2007b].
The current experiment tested whether using individually normed frequencies would improve numerical estimation of associative probabilities.
Previous studies have shown that people are generally poor at estimating what *others* would say when given a particular cue word [@Maki2007a; @Maki2007b; @Buchanan2010; @Foster2012; @maxwell2020; @maxwell2021], suggesting that collective norms are difficult to approximate.
In a traditional judgment of associative memory (JAM) task, participants estimate how many people out of 100 would produce a target word in response to a cue [@Maki2007].
In contrast, our design had participants generate their own frequency norms across multiple sessions and then judge the probability of their own cue–target pairings.
If these frequency memories function like distributed practice in study skills, repeated exposure should foster expertise in probability estimation, improving judgment capacity relative to comparison groups.

The following hypotheses were examined:

-   *Hypothesis 1*: Word frequency will be correlated with previous database frequency on an individual and overall participant level.
-   *Hypothesis 2*: Free association database norms will be predictive of all group's judgments. Simple linear regressions will be used to calculate the slope of judgments when compared to free association norms. Given previous research [@Maki2007; @Maki2007a; @Valentine2013], the values were expected to be sensitive (*b* $\ne$ 0) but not perfectly attuned (*b* = 1).
-   *Hypothesis 3*: Judgment ability will vary across groups, so that the experimental group should show better judgment ability when compared to their own norms over control, matched, and experimental groups compared to free association norms.

These hypotheses underscore how expertise, operationalized as repeated interaction with specific word pairings and their frequencies, affects the ability to make numerical judgments of probability from memory.
In studies of distributed practice, repeated exposure increases the subjective likelihood of remembering an item, leading to higher judgments of learning.
A parallel process can be expected here: as items are encountered repeatedly, participants should assign higher probability values to those associations, reflecting strengthened memory connections.
Thus, expertise not only improves memory retention but also has the potential to enhance the calibration of numerical estimates, linking metacognitive monitoring with fundamental processes in numerical cognition.

```{r create-data, include = FALSE, eval = FALSE, echo = FALSE}
##read the two data sets and norms
control <- read.csv("../2_Data/all_group_judgments/control data.csv")
exp <- read.csv("../2_Data/all_group_judgments/exp data.csv")
nelson <- read.delim("../2_Data/all_group_judgments/usf_norms.txt")
swow <- read.csv("../2_Data/all_group_judgments/strength.SWOW-EN.R123.csv")

##combine those together
control.long <- melt(control, 
                    id <- "Record")
control.long[ , c("cue", "target")] <- matrix(unlist(
  strsplit(as.character(control.long$variable), 
           ".", fixed = TRUE)), ncol = 2, byrow = TRUE)
control.long$group <- "control"
control.long$matched.partno <- NA
control.long$expnorm <- NA
control.long$fsg <- NA
control.long$bsg <- NA
control.long$swow <- NA
colnames(control.long)[c(1,3)] <- c("partno", "judgment")

fulldata <- rbind(exp, control.long[ , -2])

fulldata$cue <- tolower(fulldata$cue)
fulldata$target <- tolower(fulldata$target)
nelson$CUE <- tolower(nelson$CUE)
nelson$TARGET <- tolower(nelson$TARGET)
nelson$BSG <- as.numeric(as.character(nelson$BSG))

for (i in 1:nrow(fulldata)){

  ##add nelson FSG  
  if (length(nelson$FSG[nelson$CUE == fulldata$cue[i] 
                        & nelson$TARGET == fulldata$target[i]]) > 0)
    {
      fulldata$fsg[i] <- nelson$FSG[nelson$CUE == fulldata$cue[i] & nelson$TARGET == fulldata$target[i]]
  } else {
      fulldata$fsg[i] <- NA
  }
  
  ##add nelson BSG
  if (length(nelson$BSG[nelson$CUE == fulldata$cue[i] 
                        & nelson$TARGET == fulldata$target[i]]) > 0)
    {
      fulldata$bsg[i] <- nelson$BSG[nelson$CUE == fulldata$cue[i] & nelson$TARGET == fulldata$target[i]]
  } else {
      fulldata$bsg[i] <- NA
  }
  
  ##add swow 
  if (length(swow$R123.Strength[swow$cue == fulldata$cue[i] 
                        & swow$response == fulldata$target[i]]) > 0)
    {
      fulldata$swow[i] <- swow$R123.Strength[swow$cue == fulldata$cue[i] 
                        & swow$response == fulldata$target[i]]
  } else {
      fulldata$swow[i] <- NA
  }
  
}

##make all data in the 100 scale
fulldata$expnorm <- fulldata$expnorm * 20
fulldata$judgment <- fulldata$judgment * 20
fulldata$fsg <- fulldata$fsg * 100
fulldata$bsg <- fulldata$bsg * 100
fulldata$swow <- fulldata$swow *100

write.csv(fulldata, "../2_Data/judgmentdata.csv", row.names = FALSE)
```

# Method

## Participants

```{r load-data, include=FALSE, echo=FALSE}
##the progress track guide includes all experimental participants and how much they completed in the experiment -> this file is under exp set up folder
##the matched group was collected until there was a pair for every person
##the control group n can be seen by importing the dataset and looking at unique participant numbers

fulldata <- read.csv("../2_Data/judgmentdata.csv")
SS <- table(fulldata$group[!duplicated(fulldata$partno)])
```

Participants were recruited through the Department of Psychology’s undergraduate subject pool at a large Midwestern university.
Students were required to participate in research for their general psychology course, and some upper-level courses allowed research participation for extra credit.
The research project was displayed on the SONA system, an online participant-credit management platform, and participants selected studies to complete based on availability and interest in the posted abstract.
The entire experiment was completed online, with each section lasting approximately five to fifteen minutes.
In the experimental group, *n* = 51 participants began the study, with *n* = `r SS["exp"]` completing all experimental sessions.
For the non-finishing group (*n* = 14), the average number of sessions was *M* = 2.14 (*SD* = 1.17), with a range of one to four rating sessions.
The comparison groups included `r SS["control"]` participants for the control group and `r SS["matched"]` participants for the matched group.

## Materials

```{r exp-stim, include = FALSE, warning=FALSE}
exp_stim <- import("../1_Materials/experimental/experimental cues.xlsx")

smallstim <- subset(exp_stim, `Cue Set Size` < 10)
largestim <- subset(exp_stim, `Cue Set Size` > 10)

smallMcss <- mean(smallstim$`Cue Set Size`)
smallSDcss <- sd(smallstim$`Cue Set Size`)
range(smallstim$`Cue Set Size`)
smallMfsg <- mean(smallstim$`Forward Strength`)
smallSDfsg <- sd(smallstim$`Forward Strength`)
smallMbsg <- mean(as.numeric(smallstim$`Backward Strength`), na.rm  = T)
smallSDbsg <- sd(as.numeric(smallstim$`Backward Strength`), na.rm  = T)

largeMcss <- mean(largestim$`Cue Set Size`)
largeSDcss <- sd(largestim$`Cue Set Size`)
range(largestim$`Cue Set Size`)
largeMfsg <- mean(largestim$`Forward Strength`)
largeSDfsg <- sd(largestim$`Forward Strength`)
largeMbsg <- mean(as.numeric(largestim$`Backward Strength`), na.rm  = T)
largeSDbsg <- sd(as.numeric(largestim$`Backward Strength`), na.rm  = T)
```

Stimuli were selected from the free association word norms by @Nelson2004.
The database includes a list of cues shown to participants, with the responses given by participants in their study.
For example, with the pair *steak-sirloin*, *steak* is the cue word that is paired with the target word, *sirloin*.
Each cue word (the first word) has several different target words (*steak-cow*, *steak-sauce*).
Cue words were selected with varying number of target combinations, specifically, ten cue words with small cue set sizes and ten cues with large cue set sizes.
Cue set size indicates the number of other pairs in the database; for example, *car* has 25 cue-target combinations in the @Nelson2004 database, while *pupil* only has four cue-target combinations.

The forward strength (FSG) indicates the likelihood of the the response, given the cue ($P(response|cue)$), while backward strength (BSG) indicates the reverse probability ($P(cue|response$)).
Free association probability is not symmetric, and therefore, FSG $\ne$ BSG in most cue-response pairs.
The ten cue words with a smaller cue set size ($M_{Set Size}$ = `r printnum(smallMcss)`, $SD_{Set Size}$ = `r printnum(smallSDcss)`, range = 3-5) had an average forward strength of $M_{FSG}$ = `r printnum(smallMfsg, gt1 = F)` ($SD_{FSG}$ = `r printnum(smallSDfsg, gt1 = F)`) and backward strength of $M_{BSG}$ = `r printnum(smallMbsg, gt1 = F)` (*SD* = `r printnum(smallSDbsg, gt1 = F)`).
The larger cue set size words ($M_{Set Size}$ = `r printnum(largeMcss)`, $SD_{Set Size}$ = `r printnum(largeSDcss)`, range = 20-33) had a forward strength of $M_{FSG}$ = `r printnum(largeMfsg, gt1 = F)` ($SD_{FSG}$ = `r printnum(largeSDfsg, gt1 = F)`) and a backwards strength of $M_{BSG}$ = `r printnum(largeMbsg, gt1 = F)` ($SD_{BSG}$ = `r printnum(largeSDbsg, gt1 = F)`).
Target word selection is described below.
The complete set of materials can be found at <https://github.com/doomlab/jam-numeracy-longitudinal>.

## Procedure

### Experimental Group

#### Norming Phase

This group of participants was given the opportunity to compare their own pairing probabilities rather than estimating others’ likely judgments.
In the norming stage, participants received instructions for a free association task, described as writing down "the first word that pops into your mind when you hear a cue word." For example, many people may associate *cat* with *dog* because of common ownership, but they may also produce idiomatic responses such as it’s raining cats and dogs.
These examples emphasized free association as reflecting general language use rather than limited to literal features (e.g., *fur*, *tails*, *whiskers*).
After these instructions, participants were presented with twenty cue words, each accompanied by four blanks.
For each cue word, they wrote the first four target words that came to mind, providing variation in the target responses during the initial stage.
All responses were stored for later use.

After a minimum delay of two days, participants were invited to complete the survey again.
Email reminders were sent when the next session became available.
Each participant completed the survey five times, with cue words randomized at each presentation.
Responses across the five sessions were then averaged to generate probabilities for each cue–target pairing, following procedures similar to those used in the free association database [@Nelson2004].
For example, across five sessions, a participant might generate several different responses to the cue *computer*, such as *mouse*, *screen*, *game*, *program*, *keyboard*, or *data*.
Each cue–target probability reflected the proportion of sessions in which that target was produced (e.g., if screen was listed in all five sessions, its probability was 5/5, or 100%).
From these data, 50 cue–target combinations were selected for each participant, with ten word–target pairs drawn from each probability level (20%, 40%, 60%, 80%, and 100%).

#### Judgment Phase

Participants were then asked to estimate the probability of each of their cue–target combinations.
For example, a participant might see the prompt: “When asked about *computer*, you listed the word *program*.
What percent of the time did you put computer and program together?”
Responses were made on a rating scale with five options (20%, 40%, 60%, 80%, and 100%) by selecting the appropriate radio button.
After completing the final survey, participants were debriefed.The complete dataset of cue–target responses and probability judgments from both phases, along with an R Markdown analysis file created using the *papaja* package [@Aust2017], is available at our GitHub repository: <https://github.com/doomlab/jam-numeracy-longitudinal>.

### Control Group

```{r control-stim, include=FALSE}
##these files can be found in the exp and control set up folder along with the instructions, consent, etc. that are described. 
fulldata$index <- paste(fulldata$cue, fulldata$target, sep = "")
controlgroup <- subset(fulldata, group == "control")
controlwords <- controlgroup[!duplicated(controlgroup$index), ]

##fsg words remember we multiplied by 100 to match the scale
mean(controlwords$fsg); sd(controlwords$fsg)
##bsg words
mean(controlwords$bsg, na.rm = T); sd(controlwords$bsg, na.rm = T)
```

Results from a separate control group were compared with the experimental participants’ judgment scores.
Because each experimental participant’s final word pairs were unique, a set of cue–target pairings was selected from the free association database [@Nelson2004] to serve as a comparison.
The same twenty cue words were used, with target words chosen to ensure an equal distribution of low-, medium-, and high-strength associations.
For each cue word from the experimental norming phase, three cue–target pairs were selected, yielding a total of 60 word pairs.
Several cues were necessarily repeated to create the full set of 60 pairs, thereby matching the repetition structure used in the experimental group.
The average FSG was *M* = .19 (*SD* = .25) and the BSG was *M* = .08 (*SD* = .15).
The control group was given the same instructions about a free association task, along with examples.
Next, the rating task was explained as follows: "How many people out of a 100 would give the target (second) word when asked the cue (first) word?" Participants estimated the probability of word pair occurrence using the same 20%-40%-60%-80%-100% scale as the experimental group.

### Control Matched Group

Last, a separate comparison group was included to parallel the experimental group.
Each participant in this group was randomly paired with an experimental participant.
They received the same instructions as the control group for both the free association and rating tasks.
However, rather than judging randomly selected word–pairs, they evaluated the normed word–pairs generated by their paired experimental participant.
This matched group provided a test of stimulus effects on judgment, allowing us to determine whether improved performance in the experimental group was specifically due to participants’ prior interaction with the word–pairs.

# Results

## Experimental Norming Descriptive Statistics 

```{r descriptive-statistics}
exclude_words <- c("the", "an", "of", "a", "and", "to", "than", "that", "then")
norming <- import("../2_Data/exp_group_norming/processed_data/exp_words_processed.xlsx") %>% 
  mutate(cue = tolower(cue), 
         response = tolower(response)) %>% 
  


```

Across all five testing sessions, participants generated a large and varied set of responses.
On average, each participant listed *M* = 186.22 (*SD* = 37.66) unique target words, resulting in a total of 8,457 cue–target pairs across the experiment.
The vast majority of responses were produced only once, demonstrating that participants were not simply repeating a small set of highly accessible words, but instead generating a broad range of associations across sessions.
At the same time, a small but clear increase was observed at the five-mention level, indicating that a subset of cue–target pairs were consistently retrieved across all sessions and reflected particularly strong associations.
This dual pattern, mostly novel responses, with a stable cluster of repeated pairings, captures both the flexibility and stability of associative memory.

At the stimulus level, cues elicited an average of *M* = 162.60 (*SD* = 29.49) unique targets, underscoring the diversity of responses to each word.
When collapsed across all participants and stimuli, the experiment yielded 2,277 distinct target words.
This broad distribution suggests that free association tasks elicit a wide variety of lexical connections, while the consistent recurrence of certain responses highlights the emergence of **high-frequency, strongly linked associations**.
Together, these descriptive findings show that the experimental design successfully captured both the variability of associative networks and the stability of robust word pairings, setting the stage for later analyses of participants’ probability judgments.

\newpage

# References

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
